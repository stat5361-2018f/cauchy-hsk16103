---
title: "< STAT-5361 > HW#3-Exercises"
author: "Hee-Seung, Kim"
date: "September 21, 2018"
output: pdf_document
fontsize: 11pt
documentclass: article
knit: "bookdown::render_book('HW3_Hee.Rmd','bookdown::pdf_document2')"
papersize: letter
biblio-style: asa 
---
 
 

(a) Proof

$$f(x;\theta)=\frac{1}{\pi[1+(x-\theta)^2]},x \in R, \theta \in R. $$
Because $x_1,...,x_n$ is an i.i.d. sample and $l(\theta)$ the log-likelihood function of $\theta$ based on the sample.
Therefore,
$$\begin{aligned}
l(\theta)&=\ln(\prod_{i=1}^n f(x_i;\theta))\\
&=\ln(\prod_{i=1}^n \frac{1}{\pi[1+(x_i-\theta)^2]})\\
&=\sum_{i=1}^n\ln(\frac{1}{\pi[1+(x_i-\theta)^2]})\\
&=\sum_{i=1}^n\ln(\frac{1}{\pi})+\sum_{i=1}^n\ln(\frac{1}{1+(x_i-\theta)^2})\\
&=-n\ln\pi-\sum_{i=1}^n\ln[1+(\theta-x_i)^2]\\
\end{aligned}$$

And,
$$\begin{aligned}
l\,'(\theta)&= \frac{\partial}{\partial\theta}(-n\ln\pi-\sum_{i=1}^n\ln[1+(\theta-x_i)^2])\\
&=-\frac{\partial}{\partial\theta}(\sum_{i=1}^n\ln[1+(\theta-x_i)^2])\\
&=-(\sum_{i=1}^n\frac{2(\theta-x_i)}{1+(\theta-x_i)^2})\\
&=-2\sum_{i=1}^n\frac{\theta-x_i}{1+(\theta-x_i)^2}\\
\end{aligned}$$

$$\begin{aligned}
l\,''(\theta)&=\frac{\partial}{\partial\theta}(-2\sum_{i=1}^n\frac{\theta-x_i}{1+(\theta-x_i)^2})\\
&=-2\sum_{i=1}^n(\frac{1}{1+(\theta-x_i)^2}-\frac{2(\theta-x_i)^2}{(1+(\theta-x_i)^2)^2})\\
&=-2\sum_{i=1}^n(\frac{1+(\theta-x_i)^2}{(1+(\theta-x_i)^2)^2}-\frac{2(\theta-x_i)^2}{(1+(\theta-x_i)^2)^2})\\
&=-2\sum_{i=1}^n\frac{1-(\theta-x_i)^2}{[1+(\theta-x_i)^2]^2}\\
\end{aligned}$$
As for $I(\theta)$, 
$$
I_n(\theta)=n\int\frac{{f'(x)}^2}{f(x)}dx\\
$$


$$\begin{aligned}
I(\theta)&=n\int\frac{(f\,'(x))^2}{f(x)}dx\\
&=n\int_{-\infty}^{\infty}\frac{-(\frac{2{\pi}x}{({\pi}(1+x^2))^2})^2}{\frac{1}{{\pi}(1+x^2)}}dx=n\int_{-\infty}^{\infty}\frac{4{\pi}^2x^2}{({\pi}(1+x^2))^3}dx\\
&=\frac{4n}{\pi}\int_{-\infty}^{\infty}\frac{x^2}{(1+x^2)^3}dx=\frac{4n}{\pi}\int_{-\infty}^{\infty}\frac{x^2}{1+x^2}\frac{1}{(1+x^2)^2}dx\\
&=\frac{4n}{\pi}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\frac{\arctan^2(\theta)}{1+\arctan^2(\theta)}\frac{1+\arctan^2(\theta)}{(1+\arctan^2(\theta))^2}d\theta\\
&=\frac{4n}{\pi}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\sin^2(\theta)\cos^2(\theta)d\theta=\frac{4n}{\pi}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\frac{sin^2(2\theta)}{4}d\theta\\
&=\frac{n}{\pi}(\frac{\pi}{2})=\frac{n}{2}
\end{aligned}$$ 


\newpage
(b) Graph the log-likelihood function

```{r, echo=TRUE}
set.seed(20180909)
n = 10


data <- rcauchy(n,5,1)
in_value = seq(-10, 20, by=0.5);

m <- length(in_value)
y <- c()

Log_like=function(theta)
{
  Hee= -(length(data)*log(pi) + sum(log(1+(theta-data)^2))) #-log
  return(Hee)
}

curve(sapply(x,Log_like),-10,10,xlab=expression(theta), ylab='log-likelihood')
```

\newpage
(C)
```{r, echo=TRUE}
set.seed(20180909)
n = 10


data <- rcauchy(n,5,1)
in_value = seq(-10, 20, by=0.5);

m <- length(in_value)

first_deri=function(theta){
  first_deri=-2*sum((theta-data)/(1+(theta-data)^2))
  return(first_deri)
}
second_deri=function(theta){
  second_deri=-2*sum((1-(theta-data)^2)/(1+(theta-data)^2)^2)
  return(second_deri)
}

Newton = function(in_value,max, tol=1e-10){
  curr=in_value
  for(i in 1:max)
  {
    update=curr-first_deri(curr)/second_deri(curr)
    if(abs(update-curr)<tol) break
    curr=update
    
  }
  return(c(curr,i))
}


iii=length(in_value)
Ex_c=matrix(0,iii,2)
for(i in 1:iii){
 Ex_c[i,]=Newton(in_value[i], 200) 
  
}
  
  plot(in_value,Ex_c[,1])
```


\newpage
(d)
```{r, echo=TRUE}
  set.seed(20180909)
  n = 10
  
  
  data <- rcauchy(n,5,1)
  in_value = seq(-10, 20, by=0.5);
  
  m <- length(in_value)
  
  first_deri=function(theta){
    first_deri=-2*sum((theta-data)/(1+(theta-data)^2))
    return(first_deri)
  }

 fixed_point=function(in_value,alpha, max=1000, tol=1e-10 ){
   
   curr=in_value
   for(i in 1: max)
   {
     update=curr + (alpha*first_deri(curr))
     if (abs(update-curr)<tol) break
     curr=update
   }
return(c(curr,i))   
  
 }
   
alpha_1 <- alpha_0.64 <- alpha_0.25 <- matrix(0,length(in_value),2)

for(i in 1:length(in_value))
{
  alpha_1[i,]=fixed_point(in_value[i],1,1000)
  alpha_0.64[i,]=fixed_point(in_value[i],0.64,1000)
  alpha_0.25[i,]=fixed_point(in_value[i],0.25,1000)
}

alpha_1[,1]
alpha_0.64[,1]
alpha_0.25[,1]
```

\newpage
(e)
```{r, echo=TRUE}
set.seed(20180909)
n = 10


data <- rcauchy(n,5,1)
in_value = seq(-10, 20, by=0.5);

m <- length(in_value)

first_deri=function(theta){
  first_deri=-2*sum((theta-data)/(1+(theta-data)^2))
  return(first_deri)
}

Newton = function(in_value,max, tol=1e-10){
  curr=in_value
  for(i in 1:max)
  {
    update=curr+2*first_deri(curr)/10
    if(abs(update-curr)<tol) break
    curr=update
    
  }
  return(c(curr,i))
}


iii=length(in_value)
Ex_e=matrix(0,iii,2)
for(i in 1:iii){
  Ex_e[i,]=Newton(in_value[i], 200) 
  
}

plot(in_value,Ex_e[,1])
```


(f)
We have performed 3 iterations of the Fisher scoring method, then the Newton-Raphson method for refinement. Under Fisher scoring together with Newton method, the convergence points of MLE are less affected by choice of initial point. The number of iterations is not significantly different from the results of Newton-Raphson method alone. All iterations are similar in terms of the speed. Under those testing conditions, however, we can say that the Fisher Scoring with Newton method is the most stable algorithm for global maximization overall

